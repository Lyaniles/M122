# M122
Dies ist ein Repo f√ºr das Modul 122 der TBZ.

---
# Guide um Github Repo mit VS Code zu verbinden.


#  1. Das Remote-Repository verkn√ºpfen

Bevor du Daten √ºbertragen kannst, muss dein lokaler Ordner wissen, wohin die Reise geht. Mit dem Befehl `remote add` stellst du die Verbindung zu deinem GitHub-Repository her. Wir nennen diese Verbindung standardm√§√üig `origin`

``` Shell
 git remote add origin https://github.com/Lyaniles/M122
```

# 2. Abgleich mit dem Server (Synchronisation)

Um sicherzustellen, dass dein lokaler Stand nicht mit √Ñnderungen auf GitHub kollidiert, f√ºhren wir einen `pull` durch. Die Flag `--rebase` sorgt dabei f√ºr eine saubere Historie, indem sie deine lokalen √Ñnderungen "oben auf" die neuesten √Ñnderungen vom Server setzt, anstatt einen un√ºbersichtlichen Merge-Commit zu erstellen.

``` Shell
git pull origin main --rebase
```
> **Hinweis:** Falls dein Haupt-Branch auf GitHub anders hei√üt (z.B. `master`), passe den Namen im Befehl entsprechend an. 

# 3. Die Daten hochladen

Sobald dein lokaler Stand aktuell ist, kannst du deine Arbeit auf GitHub ver√∂ffentlichen. Mit der Flag `-u` (kurz f√ºr `--set-upstream`) speicherst du die Verbindung f√ºr die Zukunft. Das bedeutet, dass du ab jetzt einfach nur noch `git push` eingeben musst, ohne jedes Mal `origin main` zu wiederholen.

``` Shell
git push -u origin main
```

### 1. √Ñnderungen speichern (Der Standard-Zyklus)

Wenn du Dateien bearbeitet hast, musst du diese ‚Äûstagen‚Äú (vormerken) und dann ‚Äûcommitten‚Äú (festschreiben).

- **Status pr√ºfen:** Schau nach, welche Dateien ge√§ndert wurden:
    ``` Shell
    git status
    ```
    
- **Dateien hinzuf√ºgen:** Alle √Ñnderungen f√ºr den n√§chsten Commit vormerken:
    ```Sh
    git add .
    ```
    
- **√Ñnderungen festschreiben:** Erstelle einen Snapshot deiner Arbeit mit einer kurzen Nachricht:
    ```Sh
    git commit -m "Hier beschreiben, was du gemacht hast"
    ```
    
- **Hochladen:** Deine Commits an GitHub senden:
    ```Sh
    git push
    ```

### 2. Mit Branches arbeiten (Parallel arbeiten)

Falls du ein neues Feature ausprobieren willst, ohne den Hauptcode (`main`) zu gef√§hrden, solltest du einen eigenen Zweig (Branch) verwenden.

- **Neuen Branch erstellen:**
    ```Sh
    git checkout -b mein-neues-feature
    ```
    
- **Zwischen Branches wechseln:**
    ```Sh
    git checkout main
    ```
    
- **Branches zusammenf√ºhren (Merge):** Wenn dein Feature fertig ist, wechselst du zur√ºck in den `main` und holst dir die √Ñnderungen:
    ```Sh
    git checkout main
    git merge mein-neues-feature
    ```
    

---

# üöÄ Gemini Lead Scraper Product

This repository contains a professional-grade web scraper built for **Module 122**. It automates the process of finding business leads (Title, URL, Description) from Google Search results using the Brave Browser and saves them to a MariaDB database.

## ‚ú® Key Features

- **Interactive UI:** Built with **Streamlit** for a modern, easy-to-use experience. No coding knowledge required to run searches.
- **Robust Automation:** Uses **Selenium** to navigate Google, handle cookies, and manage pagination automatically.
- **Database Integration:**
  - **MariaDB Support:** Automatically stores all scraped leads in a local or remote MariaDB 10.11+ database.
  - **Auto-Deduplication:** Prevents duplicate URLs from entering the database.
  - **History View:** View and export your entire scraping history directly from the UI.
- **Anti-Detection:** 
  - Implements a **Persistent User Profile** to maintain Google "trust" and drastically reduce Captchas.
  - Randomizes delays and mimics human behavior.
- **Data Quality:**
  - **Enrichment:** Extracts Title, URL, and the Snippet Description.
  - **Smart Export:** Saves CSV files with unique timestamps (`leads_YYYYMMDD-HHMMSS.csv`) to prevent file locking errors.
- **Error Handling:** Automatically captures debug screenshots if Google blocks the request or changes layout.

## üõ†Ô∏è Installation & Usage

### Prerequisites
1.  **Python 3.10+** installed.
2.  **Brave Browser** installed (Path is configurable in the UI).
3.  **MariaDB Server (10.11+)** installed and running.
    - The application will automatically create the `google_scraper` database if the user has permissions.

### Quick Start (Windows)
Double-click the `start_app.bat` file. 
This script will:
1.  Create a virtual environment (`venv`).
2.  Install dependencies (`requirements.txt`).
3.  Launch the User Interface in your default browser.

### Manual Start
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
python -m streamlit run app.py
```

## ‚öôÔ∏è Configuration
The settings are managed via the UI but saved to `config.json` for persistence.
- **Database:**
  - `db_host`: Database host (default: `localhost`)
  - `db_user`: Database user (default: `root`)
  - `db_password`: Database password (leave empty if none)
  - `db_name`: Database name (default: `google_scraper`)
- **Scraper:**
  - **Brave Browser Path:** Location of your `brave.exe`.
  - **Headless Mode:** Run without opening a visible window (Uncheck this to solve Captchas manually).
  - **Pages:** How deep to scrape (1-10 pages).

## üìÇ Project Structure
- `app.py`: The frontend application (Streamlit).
- `search.py`: The backend scraping logic (`GoogleScraper` class).
- `database.py`: Handles MariaDB connection and data insertion.
- `start_app.bat`: One-click launcher for stakeholders.
- `automation_profile/`: Folder storing browser cookies/session (Do not delete if you want to avoid Captchas).
